# Multi-Layer-Perceptron-from-scratch-
This code is for understanding how nueral network works. Different optimizers are included in this for developing
a good intuition about how these work. 
Optimizers: Gradient Descent,AdaGrad,RMSProp,Adam.
Activation functions: Sigmoid,Tanh,Rectified Linear Unit(ReLU),Leaky ReLU.
Initialization: Random initialization, Zero, Xavier, He.
Random Initialization: All weights and biases initialized randomly.
Zero: All weights and biases initialized to zero. This is worse kind of initialization and it leads to no learning of weights and biases.
Xavier: Most popular type of initialization for Sigmoid and tanh activation functions. Weights are initialized as inversely proportional to the number of nuerons in lower layer.
He: Most popular type of initialization for relu and leaky relu activation functions. Weights are initialized as inversely proportional to half the number of nuerons in lower layer.
